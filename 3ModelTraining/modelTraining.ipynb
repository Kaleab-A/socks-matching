{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-29 02:32:21.212824: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-29 02:32:21.537509: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-29 02:32:21.545450: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-29 02:32:21.545486: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-29 02:32:23.388122: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-29 02:32:23.388244: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-29 02:32:23.388255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32, 64, 128, 256]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.0, 0.15, 0.3, 0.45]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'tanh']))\n",
    "HP_LR = hp.HParam('learning_rate', hp.Discrete([0.0001, 0.001, 0.01, 0.1]))\n",
    "HP_NUM_LAYERS = hp.HParam('num_layers', hp.Discrete([0, 1, 2, 3, 4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(config):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(config[HP_NUM_UNITS], activation=config[HP_ACTIVATION], input_shape=(7*7*1280,)))\n",
    "\n",
    "  for i in range(config[HP_NUM_LAYERS]):\n",
    "    model.add(Dense(config[HP_NUM_UNITS], activation=config[HP_ACTIVATION]))\n",
    "\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  if config[HP_OPTIMIZER] == 'adam':\n",
    "    optimizer = Adam(learning_rate=config[HP_LR])\n",
    "  elif config[HP_OPTIMIZER] == 'sgd':\n",
    "    optimizer = SGD(learning_rate=config[HP_LR])\n",
    "  else:\n",
    "    optimizer = RMSprop(learning_rate=config[HP_LR])\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance \n",
    "def linear_diff(np1, np2):\n",
    "  np1 = np1.flatten()\n",
    "  np2 = np2.flatten()\n",
    "\n",
    "  return np1 - np2\n",
    "\n",
    "# TODO - also try absolute difference, cosine similarity, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = np.load(\"./train_X.npy\")\n",
    "# trainY = np.load(\"./train_y.npy\")\n",
    "\n",
    "# print(features.shape)\n",
    "# trainX = []\n",
    "# for i in range(0, len(features)):\n",
    "#   trainX.append(linear_diff(features[i][0], features[i][1]))\n",
    "\n",
    "# trainX = np.array(trainX)\n",
    "# print(trainX.shape)\n",
    "# print(trainY.shape)\n",
    "\n",
    "\n",
    "# # Shuffle\n",
    "# indices = np.arange(trainX.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# trainX = trainX[indices]  \n",
    "# trainY = trainY[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the eud version\n",
    "\n",
    "# np.save(\"./train_X_eud.npy\", trainX)\n",
    "# np.save(\"./train_y_eud.npy\", trainY)\n",
    "\n",
    "# Load the eud version\n",
    "trainX = np.load(\"./train_X_eud.npy\")\n",
    "trainY = np.load(\"./train_y_eud.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringifyConfig(config):\n",
    "  return f\"numLayers-{config[HP_NUM_LAYERS]}_numUnits-{config[HP_NUM_UNITS]}_dropout-{config[HP_DROPOUT]}_optimizer-{config[HP_OPTIMIZER]}_activation-{config[HP_ACTIVATION]}_lr-{config[HP_LR]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "modelID:  1703838767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-29 02:32:47.660208: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-12-29 02:32:47.660291: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-12-29 02:32:47.660349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (blankk): /proc/driver/nvidia/version does not exist\n",
      "2023-12-29 02:32:47.661757: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  1003553\n",
      "HP_NUM_UNITS:  16\n",
      "HP_NUM_LAYERS:  0\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.73797, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.73797 to 1.30235, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.30235 to 0.80913, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.80913\n",
      "\n",
      "Epoch 5: val_loss improved from 0.80913 to 0.77240, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.77240 to 0.76095, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.76095 to 0.69994, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.69994 to 0.61266, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.61266 to 0.52703, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.52703 to 0.47523, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.47523 to 0.30352, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.30352 to 0.27414, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.27414 to 0.23370, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 0.23370 to 0.20787, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.20787 to 0.20128, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.20128 to 0.19817, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.19817 to 0.18409, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.18409 to 0.17895, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.17895\n",
      "\n",
      "Epoch 20: val_loss improved from 0.17895 to 0.16410, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16410\n",
      "\n",
      "Epoch 22: val_loss improved from 0.16410 to 0.16309, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16309\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16309\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16309\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16309\n",
      "\n",
      "Epoch 27: val_loss improved from 0.16309 to 0.15855, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.15855\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.15855\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.15855\n",
      "\n",
      "Epoch 31: val_loss improved from 0.15855 to 0.15314, saving model to ./models/flatten&eudlidian/numLayers-0_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838767.h5\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.15314\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.15314\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.15314\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.15314\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.15314\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.15314\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.15314\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.15314\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.15314\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.15314\n",
      "\n",
      "modelID:  1703838887\n",
      "Parameters:  1003825\n",
      "HP_NUM_UNITS:  16\n",
      "HP_NUM_LAYERS:  1\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.71527, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.71527 to 0.57729, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.57729 to 0.48779, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.48779 to 0.39399, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.39399 to 0.39072, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.39072 to 0.38518, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.38518 to 0.33688, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.33688 to 0.32668, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.32668\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.32668\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.32668\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.32668\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.32668\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.32668\n",
      "\n",
      "Epoch 15: val_loss improved from 0.32668 to 0.31585, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.31585\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.31585\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.31585\n",
      "\n",
      "Epoch 19: val_loss improved from 0.31585 to 0.28246, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.28246 to 0.26652, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.26652\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.26652\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.26652\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.26652\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.26652\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.26652\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.26652\n",
      "\n",
      "Epoch 28: val_loss improved from 0.26652 to 0.26399, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.26399 to 0.25607, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.25607 to 0.23936, saving model to ./models/flatten&eudlidian/numLayers-1_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703838887.h5\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.23936\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.23936\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.23936\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.23936\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.23936\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.23936\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.23936\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.23936\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.23936\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.23936\n",
      "\n",
      "modelID:  1703839004\n",
      "Parameters:  1004097\n",
      "HP_NUM_UNITS:  16\n",
      "HP_NUM_LAYERS:  2\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.37367, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839004.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.37367 to 0.28405, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839004.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.28405 to 0.22575, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839004.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.22575\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.22575\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.22575\n",
      "\n",
      "Epoch 7: val_loss improved from 0.22575 to 0.21112, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839004.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.21112 to 0.20942, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839004.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.20942 to 0.19013, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839004.h5\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.19013\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.19013\n",
      "\n",
      "Epoch 12: val_loss improved from 0.19013 to 0.18540, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839004.h5\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.18540\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.18540\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.18540\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.18540\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.18540\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.18540\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.18540\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.18540\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.18540\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.18540\n",
      "\n",
      "modelID:  1703839073\n",
      "Parameters:  1004369\n",
      "HP_NUM_UNITS:  16\n",
      "HP_NUM_LAYERS:  3\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.42392, saving model to ./models/flatten&eudlidian/numLayers-3_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839073.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.42392 to 0.30596, saving model to ./models/flatten&eudlidian/numLayers-3_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839073.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.30596 to 0.25275, saving model to ./models/flatten&eudlidian/numLayers-3_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839073.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.25275\n",
      "\n",
      "Epoch 5: val_loss improved from 0.25275 to 0.22140, saving model to ./models/flatten&eudlidian/numLayers-3_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839073.h5\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.22140\n",
      "\n",
      "Epoch 7: val_loss improved from 0.22140 to 0.21897, saving model to ./models/flatten&eudlidian/numLayers-3_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839073.h5\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.21897\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.21897\n",
      "\n",
      "Epoch 10: val_loss improved from 0.21897 to 0.19083, saving model to ./models/flatten&eudlidian/numLayers-3_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839073.h5\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.19083\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.19083\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.19083\n",
      "\n",
      "Epoch 14: val_loss improved from 0.19083 to 0.19074, saving model to ./models/flatten&eudlidian/numLayers-3_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839073.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.19074\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.19074\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.19074\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.19074\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.19074\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.19074\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.19074\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.19074\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.19074\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.19074\n",
      "\n",
      "modelID:  1703839136\n",
      "Parameters:  1004641\n",
      "HP_NUM_UNITS:  16\n",
      "HP_NUM_LAYERS:  4\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.34183, saving model to ./models/flatten&eudlidian/numLayers-4_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839136.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.34183 to 0.24539, saving model to ./models/flatten&eudlidian/numLayers-4_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839136.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.24539 to 0.20047, saving model to ./models/flatten&eudlidian/numLayers-4_numUnits-16_dropout-0_optimizer-adam_activation-relu_lr-0.0011703839136.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.20047\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.20047\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.20047\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHP_NUM_UNITS: \u001b[39m\u001b[38;5;124m\"\u001b[39m, NUM_UNIT)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHP_NUM_LAYERS: \u001b[39m\u001b[38;5;124m\"\u001b[39m, NUM_LAYER)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_call_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:2109\u001b[0m, in \u001b[0;36mConcreteFunction._build_call_outputs\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m   2107\u001b[0m     outputs_list[i] \u001b[38;5;241m=\u001b[39m result[j]\n\u001b[1;32m   2108\u001b[0m     j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2109\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack_sequence_as\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2110\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moutputs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/nest.py:806\u001b[0m, in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.pack_sequence_as\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpack_sequence_as\u001b[39m(structure, flat_sequence, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    694\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a given flattened sequence packed into a given structure.\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;124;03m    TypeError: `structure` is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pack_sequence_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/nest.py:678\u001b[0m, in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    675\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m flat_sequence[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m   final_index, packed \u001b[38;5;241m=\u001b[39m \u001b[43m_packed_nest_with_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mis_nested_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m final_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(flat_sequence):\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/nest.py:640\u001b[0m, in \u001b[0;36m_packed_nest_with_indices\u001b[0;34m(structure, flat, index, is_nested_fn, sequence_fn)\u001b[0m\n\u001b[1;32m    638\u001b[0m packed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    639\u001b[0m sequence_fn \u001b[38;5;241m=\u001b[39m sequence_fn \u001b[38;5;129;01mor\u001b[39;00m _sequence_like\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m _yield_value(structure):\n\u001b[1;32m    641\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_nested_fn(s):\n\u001b[1;32m    642\u001b[0m     new_index, child \u001b[38;5;241m=\u001b[39m _packed_nest_with_indices(s, flat, index, is_nested_fn,\n\u001b[1;32m    643\u001b[0m                                                  sequence_fn)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/nest.py:258\u001b[0m, in \u001b[0;36m_yield_value\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_yield_value\u001b[39m(iterable):\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m _yield_sorted_items(iterable):\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m v\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "method = \"flatten&eudlidian\"\n",
    "\n",
    "for NUM_UNIT in HP_NUM_UNITS.domain.values:\n",
    "  for NUM_LAYER in HP_NUM_LAYERS.domain.values:\n",
    "\n",
    "    config = {\n",
    "      HP_NUM_UNITS: NUM_UNIT,\n",
    "      HP_DROPOUT: 0,\n",
    "      HP_OPTIMIZER: 'adam',\n",
    "      HP_ACTIVATION: 'relu',\n",
    "      HP_LR: 0.001,\n",
    "      HP_NUM_LAYERS: NUM_LAYER\n",
    "    }\n",
    "\n",
    "    # Callbacks\n",
    "    modelID = int(time.time())\n",
    "    modelName = stringifyConfig(config) + str(modelID)\n",
    "\n",
    "    print(\"\\nmodelID: \", modelID)\n",
    "\n",
    "    # Callbacks\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=f\"./logs/{method}/{modelName}\", histogram_freq=1)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\"./models/\" + method + \"/\" + modelName + \".h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "\n",
    "    hparms = hp.KerasCallback(\"./logs/\" + method + \"/\" + modelName, config)\n",
    "\n",
    "    callbacks_list = [checkpoint, early, tensorboard_callback, hparms]\n",
    "\n",
    "    model = createModel(config)\n",
    "\n",
    "    print(\"Parameters: \", model.count_params())\n",
    "    print(\"HP_NUM_UNITS: \", NUM_UNIT)\n",
    "    print(\"HP_NUM_LAYERS: \", NUM_LAYER)\n",
    "\n",
    "    model.fit(trainX, trainY, batch_size=32, epochs=100, validation_split=0.2, callbacks=callbacks_list, shuffle=True, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = keras.models.load_model(\"./models/\" + method + \"/\" + modelName + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 62720)\n",
      "(1800,)\n",
      "Test loss: 0.3050694167613983\n",
      "Test accuracy: 0.9127777814865112\n"
     ]
    }
   ],
   "source": [
    "textY = np.load(\"./test_y.npy\") \n",
    "textX = []\n",
    "features = np.load(\"./test_X.npy\")\n",
    "for i in range(0, len(features)):\n",
    "  textX.append(linear_diff(features[i][0], features[i][1]))\n",
    "\n",
    "textX = np.array(textX)\n",
    "\n",
    "print(textX.shape)\n",
    "print(textY.shape)\n",
    "\n",
    "score = model.evaluate(textX, textY, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
