{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 16:15:08.189404: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 16:15:08.511819: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-28 16:15:08.520536: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-28 16:15:08.520568: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-28 16:15:10.105361: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-28 16:15:10.105472: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-28 16:15:10.105484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, activation='relu', input_shape=(7*7*1280,)))\n",
    "  model.add(Dense(8, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance \n",
    "def linear_diff(np1, np2):\n",
    "  np1 = np1.flatten()\n",
    "  np2 = np2.flatten()\n",
    "\n",
    "  return np1 - np2\n",
    "\n",
    "# TODO - also try absolute difference, cosine similarity, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7020, 2, 7, 7, 1280)\n",
      "(7020, 62720)\n",
      "(7020,)\n"
     ]
    }
   ],
   "source": [
    "features = np.load(\"./train_X.npy\")\n",
    "trainY = np.load(\"./train_y.npy\")\n",
    "\n",
    "print(features.shape)\n",
    "trainX = []\n",
    "for i in range(0, len(features)):\n",
    "  trainX.append(linear_diff(features[i][0], features[i][1]))\n",
    "\n",
    "trainX = np.array(trainX)\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "indices = np.arange(trainX.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trainX = trainX[indices]  \n",
    "trainY = trainY[indices]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 16:15:42.903142: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-12-28 16:15:42.903197: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-12-28 16:15:42.903239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (blankk): /proc/driver/nvidia/version does not exist\n",
      "2023-12-28 16:15:42.904892: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                4014144   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,014,673\n",
      "Trainable params: 4,014,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = createModel()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelID:  1703801748\n",
      "Epoch 1/100\n",
      "175/176 [============================>.] - ETA: 0s - loss: 1.7189 - accuracy: 0.8039\n",
      "Epoch 1: val_loss improved from inf to 0.77618, saving model to ./models/1703801748.h5\n",
      "176/176 [==============================] - 12s 61ms/step - loss: 1.7169 - accuracy: 0.8038 - val_loss: 0.7762 - val_accuracy: 0.8511\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.4186 - accuracy: 0.9010\n",
      "Epoch 2: val_loss improved from 0.77618 to 0.31665, saving model to ./models/1703801748.h5\n",
      "176/176 [==============================] - 10s 56ms/step - loss: 0.4186 - accuracy: 0.9010 - val_loss: 0.3167 - val_accuracy: 0.9259\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.9377\n",
      "Epoch 3: val_loss improved from 0.31665 to 0.29576, saving model to ./models/1703801748.h5\n",
      "176/176 [==============================] - 10s 57ms/step - loss: 0.2265 - accuracy: 0.9377 - val_loss: 0.2958 - val_accuracy: 0.9402\n",
      "Epoch 4/100\n",
      "175/176 [============================>.] - ETA: 0s - loss: 0.2140 - accuracy: 0.9495\n",
      "Epoch 4: val_loss improved from 0.29576 to 0.21780, saving model to ./models/1703801748.h5\n",
      "176/176 [==============================] - 10s 58ms/step - loss: 0.2145 - accuracy: 0.9494 - val_loss: 0.2178 - val_accuracy: 0.9416\n",
      "Epoch 5/100\n",
      "175/176 [============================>.] - ETA: 0s - loss: 0.1842 - accuracy: 0.9548\n",
      "Epoch 5: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 10s 56ms/step - loss: 0.1837 - accuracy: 0.9550 - val_loss: 0.3474 - val_accuracy: 0.9437\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9601\n",
      "Epoch 6: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 10s 58ms/step - loss: 0.1551 - accuracy: 0.9601 - val_loss: 0.3299 - val_accuracy: 0.9459\n",
      "Epoch 7/100\n",
      "175/176 [============================>.] - ETA: 0s - loss: 0.1855 - accuracy: 0.9609\n",
      "Epoch 7: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 10s 59ms/step - loss: 0.1867 - accuracy: 0.9605 - val_loss: 0.3955 - val_accuracy: 0.9473\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9637\n",
      "Epoch 8: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 60ms/step - loss: 0.1527 - accuracy: 0.9637 - val_loss: 0.2888 - val_accuracy: 0.9501\n",
      "Epoch 9/100\n",
      "175/176 [============================>.] - ETA: 0s - loss: 0.1533 - accuracy: 0.9643\n",
      "Epoch 9: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 10s 60ms/step - loss: 0.1540 - accuracy: 0.9640 - val_loss: 0.2441 - val_accuracy: 0.9530\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9631\n",
      "Epoch 10: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 14s 81ms/step - loss: 0.1401 - accuracy: 0.9631 - val_loss: 0.2655 - val_accuracy: 0.9494\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9646\n",
      "Epoch 11: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 12s 71ms/step - loss: 0.1309 - accuracy: 0.9646 - val_loss: 0.2590 - val_accuracy: 0.9537\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9660\n",
      "Epoch 12: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 12s 68ms/step - loss: 0.1301 - accuracy: 0.9660 - val_loss: 0.3946 - val_accuracy: 0.9501\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9660\n",
      "Epoch 13: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 63ms/step - loss: 0.1437 - accuracy: 0.9660 - val_loss: 0.3896 - val_accuracy: 0.9523\n",
      "Epoch 14/100\n",
      "175/176 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.9648\n",
      "Epoch 14: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 63ms/step - loss: 0.1396 - accuracy: 0.9646 - val_loss: 0.3788 - val_accuracy: 0.9509\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9667\n",
      "Epoch 15: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 12s 68ms/step - loss: 0.1172 - accuracy: 0.9667 - val_loss: 0.3388 - val_accuracy: 0.9544\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9660\n",
      "Epoch 16: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 60ms/step - loss: 0.1061 - accuracy: 0.9660 - val_loss: 0.3484 - val_accuracy: 0.9530\n",
      "Epoch 17/100\n",
      "175/176 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9657\n",
      "Epoch 17: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 62ms/step - loss: 0.1322 - accuracy: 0.9658 - val_loss: 0.3201 - val_accuracy: 0.9551\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.9672\n",
      "Epoch 18: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 12s 66ms/step - loss: 0.1212 - accuracy: 0.9672 - val_loss: 0.2948 - val_accuracy: 0.9566\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.9662\n",
      "Epoch 19: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 64ms/step - loss: 0.1171 - accuracy: 0.9662 - val_loss: 0.3790 - val_accuracy: 0.9544\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9676\n",
      "Epoch 20: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 62ms/step - loss: 0.1273 - accuracy: 0.9676 - val_loss: 0.3465 - val_accuracy: 0.9558\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9667\n",
      "Epoch 21: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 60ms/step - loss: 0.1156 - accuracy: 0.9667 - val_loss: 0.2786 - val_accuracy: 0.9558\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9681\n",
      "Epoch 22: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 64ms/step - loss: 0.1205 - accuracy: 0.9681 - val_loss: 0.2980 - val_accuracy: 0.9566\n",
      "Epoch 23/100\n",
      "175/176 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.9657\n",
      "Epoch 23: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 62ms/step - loss: 0.1388 - accuracy: 0.9656 - val_loss: 0.3675 - val_accuracy: 0.9437\n",
      "Epoch 24/100\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9669\n",
      "Epoch 24: val_loss did not improve from 0.21780\n",
      "176/176 [==============================] - 11s 64ms/step - loss: 0.1141 - accuracy: 0.9669 - val_loss: 0.3073 - val_accuracy: 0.9573\n",
      "Epoch 24: early stopping\n"
     ]
    }
   ],
   "source": [
    "modelID = int(time.time())\n",
    "method = \"flatten&eudlidian\"\n",
    "\n",
    "print(\"modelID: \", modelID)\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=f\"./logs/{method}/\" + str(modelID), histogram_freq=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./models/\" + str(modelID) + \".h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "callbacks_list = [checkpoint, early, tensorboard_callback]\n",
    "\n",
    "history = model.fit(trainX, trainY, batch_size=32, epochs=100, validation_split=0.2, callbacks=callbacks_list, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = keras.models.load_model(\"./models/\" + str(modelID) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 62720)\n",
      "(1800,)\n",
      "Test loss: 0.3440956175327301\n",
      "Test accuracy: 0.9011111259460449\n"
     ]
    }
   ],
   "source": [
    "textY = np.load(\"./test_y.npy\") \n",
    "textX = []\n",
    "features = np.load(\"./test_X.npy\")\n",
    "for i in range(0, len(features)):\n",
    "  textX.append(linear_diff(features[i][0], features[i][1]))\n",
    "\n",
    "textX = np.array(textX)\n",
    "\n",
    "print(textX.shape)\n",
    "print(textY.shape)\n",
    "\n",
    "score = model.evaluate(textX, textY, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
