{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 14:27:51.246532: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-03 14:27:51.611840: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-03 14:27:51.623700: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-01-03 14:27:51.623735: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-01-03 14:27:53.060569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-01-03 14:27:53.060681: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-01-03 14:27:53.060693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32, 64, 128, 256]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.0, 0.15, 0.3, 0.45]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'tanh']))\n",
    "HP_LR = hp.HParam('learning_rate', hp.Discrete([0.0001, 0.001, 0.01, 0.1]))\n",
    "HP_NUM_LAYERS = hp.HParam('num_layers', hp.Discrete([0, 1, 2, 3, 4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createModel(config):\n",
    "#   model = Sequential()\n",
    "#   model.add(Dense(config[HP_NUM_UNITS], activation=config[HP_ACTIVATION], input_shape=(512,)))\n",
    "#   model.add(Dropout(config[HP_DROPOUT]))\n",
    "\n",
    "#   for i in range(config[HP_NUM_LAYERS]):\n",
    "#     model.add(Dense(config[HP_NUM_UNITS], activation=config[HP_ACTIVATION]))\n",
    "#     # model.add(Dropout(config[HP_DROPOUT]))\n",
    "\n",
    "#   model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#   if config[HP_OPTIMIZER] == 'adam':\n",
    "#     optimizer = Adam(learning_rate=config[HP_LR])\n",
    "#   elif config[HP_OPTIMIZER] == 'sgd':\n",
    "#     optimizer = SGD(learning_rate=config[HP_LR])\n",
    "#   else:\n",
    "#     optimizer = RMSprop(learning_rate=config[HP_LR])\n",
    "\n",
    "#   model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "#   return model\n",
    "\n",
    "def createModel(config):\n",
    "  model = Sequential()  \n",
    "\n",
    "  dropoutVal = 0.5\n",
    "\n",
    "  # CNN models\n",
    "  model.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(7, 7, 1280)))\n",
    "  # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(dropoutVal))\n",
    "\n",
    "  model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "  # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(dropoutVal))\n",
    "\n",
    "  model.add(Conv2D(256, (3, 3), activation=\"relu\"))\n",
    "  # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(dropoutVal))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512, activation=\"relu\"))\n",
    "  model.add(Dense(32, activation=\"relu\"))\n",
    "  model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance \n",
    "def linear_diff(np1, np2):\n",
    "  return np1 - np2\n",
    "\n",
    "# TODO - also try absolute difference, cosine similarity, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13032, 2, 7, 7, 1280)\n",
      "(13032, 7, 7, 1280)\n",
      "(13032,)\n"
     ]
    }
   ],
   "source": [
    "features = np.load(\"./train_X.npy\")\n",
    "trainY = np.load(\"./train_y.npy\")\n",
    "\n",
    "print(features.shape)\n",
    "trainX = []\n",
    "for i in range(0, len(features)):\n",
    "  trainX.append(linear_diff(features[i][0], features[i][1]))\n",
    "\n",
    "trainX = np.array(trainX)\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./train_X_diff.npy\", trainX)\n",
    "np.save(\"./train_y_diff.npy\", trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2592, 7, 7, 1280)\n"
     ]
    }
   ],
   "source": [
    "features_val = np.load(\"./val_X.npy\")\n",
    "valY = np.load(\"./val_y.npy\")\n",
    "\n",
    "valX = []\n",
    "for i in range(0, len(features_val)):\n",
    "  valX.append(linear_diff(features_val[i][0], features_val[i][1]))\n",
    "\n",
    "valX = np.array(valX)\n",
    "print(valX.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./val_X_diff.npy\", valX) \n",
    "np.save(\"./val_y_diff.npy\", valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diff version\n",
    "trainX = np.load(\"./train_X_diff.npy\")\n",
    "trainY = np.load(\"./train_y_diff.npy\")\n",
    "valX = np.load(\"./val_X_diff.npy\")\n",
    "valY = np.load(\"./val_y_diff.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5792, 512), (5792,), (1152, 512), (1152,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, valX.shape, valY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringifyConfig(config):\n",
    "  return f\"numLayers-{config[HP_NUM_LAYERS]}_numUnits-{config[HP_NUM_UNITS]}_dropout-{config[HP_DROPOUT]}_optimizer-{config[HP_OPTIMIZER]}_activation-{config[HP_ACTIVATION]}_lr-{config[HP_LR]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "modelID:  1704313741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 14:29:01.502107: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-01-03 14:29:01.502142: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-01-03 14:29:01.502206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (blankk): /proc/driver/nvidia/version does not exist\n",
      "2024-01-03 14:29:01.503291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  682913\n",
      "HP_NUM_UNITS:  16\n",
      "HP_NUM_LAYERS:  0\n",
      "Epoch 1/100\n",
      "408/408 [==============================] - ETA: 0s - loss: 0.4832 - accuracy: 0.7405\n",
      "Epoch 1: val_loss improved from inf to 0.52177, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-256_dropout-0.5_optimizer-adam_activation-relu_lr-0.0011704313741.h5\n",
      "408/408 [==============================] - 26s 55ms/step - loss: 0.4832 - accuracy: 0.7405 - val_loss: 0.5218 - val_accuracy: 0.7346\n",
      "Epoch 2/100\n",
      "407/408 [============================>.] - ETA: 0s - loss: 0.2330 - accuracy: 0.9192\n",
      "Epoch 2: val_loss improved from 0.52177 to 0.40762, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-256_dropout-0.5_optimizer-adam_activation-relu_lr-0.0011704313741.h5\n",
      "408/408 [==============================] - 21s 52ms/step - loss: 0.2334 - accuracy: 0.9191 - val_loss: 0.4076 - val_accuracy: 0.7998\n",
      "Epoch 3/100\n",
      "408/408 [==============================] - ETA: 0s - loss: 0.1635 - accuracy: 0.9438\n",
      "Epoch 3: val_loss improved from 0.40762 to 0.36804, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-256_dropout-0.5_optimizer-adam_activation-relu_lr-0.0011704313741.h5\n",
      "408/408 [==============================] - 20s 50ms/step - loss: 0.1635 - accuracy: 0.9438 - val_loss: 0.3680 - val_accuracy: 0.8237\n",
      "Epoch 4/100\n",
      "408/408 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9629\n",
      "Epoch 4: val_loss did not improve from 0.36804\n",
      "408/408 [==============================] - 20s 50ms/step - loss: 0.1166 - accuracy: 0.9629 - val_loss: 0.5268 - val_accuracy: 0.7751\n",
      "Epoch 5/100\n",
      "408/408 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9715\n",
      "Epoch 5: val_loss did not improve from 0.36804\n",
      "408/408 [==============================] - 20s 49ms/step - loss: 0.0920 - accuracy: 0.9715 - val_loss: 0.4008 - val_accuracy: 0.8106\n",
      "Epoch 6/100\n",
      "407/408 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.9735\n",
      "Epoch 6: val_loss did not improve from 0.36804\n",
      "408/408 [==============================] - 26s 65ms/step - loss: 0.0856 - accuracy: 0.9735 - val_loss: 0.4492 - val_accuracy: 0.7955\n",
      "Epoch 7/100\n",
      "407/408 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9791\n",
      "Epoch 7: val_loss did not improve from 0.36804\n",
      "408/408 [==============================] - 24s 59ms/step - loss: 0.0694 - accuracy: 0.9791 - val_loss: 0.4918 - val_accuracy: 0.7836\n",
      "Epoch 8/100\n",
      "407/408 [============================>.] - ETA: 0s - loss: 0.0548 - accuracy: 0.9839\n",
      "Epoch 8: val_loss did not improve from 0.36804\n",
      "408/408 [==============================] - 30s 73ms/step - loss: 0.0548 - accuracy: 0.9839 - val_loss: 0.5666 - val_accuracy: 0.7959\n",
      "Epoch 9/100\n",
      "407/408 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9808\n",
      "Epoch 9: val_loss did not improve from 0.36804\n",
      "408/408 [==============================] - 29s 71ms/step - loss: 0.0675 - accuracy: 0.9807 - val_loss: 0.8954 - val_accuracy: 0.7674\n",
      "Epoch 10/100\n",
      "407/408 [============================>.] - ETA: 0s - loss: 0.0558 - accuracy: 0.9853\n",
      "Epoch 10: val_loss did not improve from 0.36804\n",
      "408/408 [==============================] - 28s 68ms/step - loss: 0.0558 - accuracy: 0.9853 - val_loss: 0.7650 - val_accuracy: 0.7604\n",
      "Epoch 11/100\n",
      "407/408 [============================>.] - ETA: 0s - loss: 0.0409 - accuracy: 0.9877\n",
      "Epoch 11: val_loss did not improve from 0.36804\n",
      "408/408 [==============================] - 29s 71ms/step - loss: 0.0409 - accuracy: 0.9877 - val_loss: 0.5196 - val_accuracy: 0.8160\n",
      "Epoch 12/100\n",
      "408/408 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9886\n",
      "Epoch 12: val_loss did not improve from 0.36804\n",
      "408/408 [==============================] - 25s 60ms/step - loss: 0.0402 - accuracy: 0.9886 - val_loss: 0.7858 - val_accuracy: 0.7600\n",
      "Epoch 13/100\n",
      "407/408 [============================>.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9917\n",
      "Epoch 13: val_loss did not improve from 0.36804\n",
      "408/408 [==============================] - 25s 61ms/step - loss: 0.0323 - accuracy: 0.9917 - val_loss: 0.7155 - val_accuracy: 0.7782\n"
     ]
    }
   ],
   "source": [
    "method = \"flatten&eudlidian\"\n",
    "\n",
    "for NUM_UNIT in HP_NUM_UNITS.domain.values:\n",
    "  for NUM_LAYER in HP_NUM_LAYERS.domain.values:\n",
    "\n",
    "    # config = {\n",
    "    #   HP_NUM_UNITS: NUM_UNIT,\n",
    "    #   HP_DROPOUT: 0,\n",
    "    #   HP_OPTIMIZER: 'adam',\n",
    "    #   HP_ACTIVATION: 'relu',\n",
    "    #   HP_LR: 0.001,\n",
    "    #   HP_NUM_LAYERS: NUM_LAYER\n",
    "    # }\n",
    "\n",
    "    config = {\n",
    "      HP_NUM_UNITS: 256,\n",
    "      HP_DROPOUT: 0.5,\n",
    "      HP_OPTIMIZER: 'adam',\n",
    "      HP_ACTIVATION: 'relu',\n",
    "      HP_LR: 0.001,\n",
    "      HP_NUM_LAYERS: 2\n",
    "    }\n",
    "\n",
    "    # Callbacks\n",
    "    modelID = int(time.time())\n",
    "    modelName = stringifyConfig(config) + str(modelID)\n",
    "\n",
    "    print(\"\\nmodelID: \", modelID)\n",
    "\n",
    "    # Callbacks\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=f\"./logs/{method}/{modelName}\", histogram_freq=1)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\"./models/\" + method + \"/\" + modelName + \".h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "\n",
    "    hparms = hp.KerasCallback(\"./logs/\" + method + \"/\" + modelName, config)\n",
    "\n",
    "    callbacks_list = [checkpoint, early, tensorboard_callback, hparms]\n",
    "\n",
    "    model = createModel(config)\n",
    "\n",
    "    print(\"Parameters: \", model.count_params())\n",
    "    print(\"HP_NUM_UNITS: \", NUM_UNIT)\n",
    "    print(\"HP_NUM_LAYERS: \", NUM_LAYER)\n",
    "\n",
    "\n",
    "    model.fit(trainX, trainY, batch_size=32, epochs=100, callbacks=callbacks_list, shuffle=True, verbose=1, validation_data=(valX, valY))\n",
    "\n",
    "    break\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = keras.models.load_model(\"./models/\" + method + \"/\" + modelName + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2916, 7, 7, 1280)\n",
      "(2916,)\n"
     ]
    }
   ],
   "source": [
    "textY = np.load(\"./test_y.npy\") \n",
    "textX = []\n",
    "features = np.load(\"./test_X.npy\")\n",
    "for i in range(0, len(features)):\n",
    "  textX.append(linear_diff(features[i][0], features[i][1]))\n",
    "\n",
    "textX = np.array(textX)\n",
    "\n",
    "print(textX.shape)\n",
    "print(textY.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 2s 20ms/step - loss: 0.4715 - accuracy: 0.7850\n",
      "Test loss: 0.47152179479599\n",
      "Test accuracy: 0.7849794030189514\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(textX, textY)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5792, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.94228   ,  4.2660823 , 11.17209   , 23.513826  ,  2.9565973 ,\n",
       "        6.5381765 ,  0.        ,  5.5042477 ,  0.        ,  1.1767998 ,\n",
       "        7.3116856 ,  0.        ,  2.3247094 ,  0.        ,  5.8922615 ,\n",
       "       17.017056  , 26.68887   ,  5.6589365 ,  1.4295353 ,  6.4644775 ,\n",
       "        4.9205627 ,  7.2869186 , 23.883442  , 18.176207  , 15.436508  ,\n",
       "       30.406403  ,  8.54992   , 33.56736   ,  1.7889771 ,  7.728585  ,\n",
       "        1.51395   , 23.54344   , 17.904615  ,  3.665018  , 13.18933   ,\n",
       "       28.25327   , 18.471085  ,  0.39198163,  3.855914  ,  0.        ,\n",
       "       32.565075  ,  1.9633389 ,  6.242937  , 91.320526  , 11.824903  ,\n",
       "       44.20205   ,  7.252417  , 28.722588  ,  4.6957383 ,  3.3829384 ,\n",
       "       11.56041   ,  0.        , 24.655628  , 17.028023  ,  0.99671066,\n",
       "       62.72991   ,  7.6214714 , 25.34861   , 11.781673  , 10.941526  ,\n",
       "        0.        ,  3.4047937 ,  3.5542145 , 18.49507   ,  3.4157085 ,\n",
       "       25.44946   , 20.102911  ,  0.        ,  6.6191225 ,  1.7154732 ,\n",
       "        0.        ,  4.142886  ,  2.8933544 , 25.894176  ,  7.43201   ,\n",
       "        1.1704848 , 21.479448  , 11.808691  ,  0.        ,  0.22007349,\n",
       "        1.267875  , 19.315876  , 13.596565  ,  8.593424  , 18.254007  ,\n",
       "       15.109403  ,  5.788501  , 29.317444  , 10.900007  ,  0.        ,\n",
       "        4.601423  , 32.08773   , 12.241854  ,  5.6557155 , 19.135052  ,\n",
       "       11.046041  , 38.367287  ,  8.256937  ,  3.8565903 , 25.887829  ,\n",
       "        0.        ,  0.        ,  3.6769524 ,  5.952099  , 20.36351   ,\n",
       "        0.        ,  7.1709595 ,  3.2328372 , 10.779606  ,  2.604168  ,\n",
       "       22.581861  , 13.880266  , 92.81866   ,  5.8674736 , 20.35466   ,\n",
       "        0.        , 15.162228  ,  2.7279282 ,  2.2879486 ,  7.0605736 ,\n",
       "        0.        , 51.32019   ,  3.6929874 , 18.529032  ,  9.279337  ,\n",
       "       10.741697  ,  9.576185  ,  0.3234511 ,  0.79195243,  2.1822257 ,\n",
       "       11.857704  , 11.723854  , 14.440509  , 18.719309  ,  3.6079338 ,\n",
       "        0.        , 39.869446  , 13.660877  ,  2.6506338 ,  0.        ,\n",
       "       25.01062   , 14.521879  , 13.141552  ,  0.        ,  9.801101  ,\n",
       "        9.96746   ,  4.7778435 ,  7.728179  ,  0.        ,  3.1377678 ,\n",
       "        0.        , 14.930876  , 16.159004  ,  7.959402  ,  5.717705  ,\n",
       "        5.101198  ,  1.568367  , 10.305203  ,  3.4532902 , 14.449188  ,\n",
       "       15.515455  ,  1.5235014 ,  4.1083994 ,  1.3304906 ,  0.73474884,\n",
       "        0.        , 12.829963  ,  5.2164593 ,  4.9114037 ,  5.896982  ,\n",
       "       20.470974  , 23.334738  ,  0.66880417, 24.48993   , 19.06252   ,\n",
       "        3.219655  , 13.592543  , 21.416887  ,  6.535043  ,  2.4972744 ,\n",
       "       19.992641  ,  0.        ,  9.645345  , 13.515648  ,  0.        ,\n",
       "        9.812567  ,  9.57703   , 15.918484  ,  2.5608122 ,  0.        ,\n",
       "       18.848282  ,  2.2927923 , 39.52236   , 10.765228  , 20.402538  ,\n",
       "        0.        , 34.478     ,  0.        ,  0.        , 15.432451  ,\n",
       "        9.761804  ,  7.2478538 ,  0.75217915,  7.0219183 ,  6.801376  ,\n",
       "        0.        ,  4.649762  , 26.630602  ,  0.        ,  4.32774   ,\n",
       "        0.        ,  8.02832   ,  2.4332657 , 12.9519825 ,  2.3026876 ,\n",
       "        1.7726612 ,  5.934012  ,  2.7317114 ,  0.38652802, 32.292404  ,\n",
       "       31.848719  ,  6.430542  , 19.366478  , 17.700142  ,  9.760252  ,\n",
       "        0.        ,  1.03829   , 15.806753  , 17.374912  ,  8.7105255 ,\n",
       "        1.2297268 ,  0.26709747,  0.        , 10.69249   ,  0.        ,\n",
       "        2.1227443 ,  8.694714  ,  9.020752  ,  5.83274   ,  5.6250734 ,\n",
       "        0.5223198 , 10.1935005 , 27.23284   ,  3.6608849 ,  3.2897227 ,\n",
       "        7.883582  ,  0.        , 36.430153  ,  5.2110577 , 37.357384  ,\n",
       "       16.11049   ,  6.450758  , 13.0319395 , 10.606108  ,  0.2679062 ,\n",
       "        2.2243462 , 16.526749  , 12.567184  , 16.85346   , 12.845848  ,\n",
       "        2.1896553 ,  0.        ,  0.        ,  2.8085213 , 21.100918  ,\n",
       "       47.516117  ,  9.144258  ,  2.725994  ,  0.        ,  0.        ,\n",
       "       42.89824   , 18.782438  ,  9.667012  , 42.35712   , 20.74937   ,\n",
       "        4.5138664 ,  6.885166  , 13.542593  , 19.820997  ,  3.366249  ,\n",
       "        2.477921  , 12.437643  , 17.365536  ,  0.5202694 , 16.551455  ,\n",
       "        1.8957863 ,  8.96841   ,  3.9791203 , 25.467781  ,  0.        ,\n",
       "        3.3100853 , 55.00757   , 33.751976  , 10.590488  , 39.775417  ,\n",
       "        3.5133839 ,  3.5118904 , 24.405975  ,  3.717537  ,  8.3183565 ,\n",
       "       14.645397  , 59.202705  , 12.89091   ,  0.51483035,  2.1952171 ,\n",
       "       35.870594  , 14.42797   , 17.385511  ,  0.        ,  8.5457735 ,\n",
       "        6.5282116 , 20.373314  ,  0.        , 13.786016  ,  0.        ,\n",
       "        6.7681675 , 33.896347  , 28.118855  ,  0.8113474 , 10.361937  ,\n",
       "       23.13121   ,  1.5052872 , 28.887829  , 15.102245  ,  3.64413   ,\n",
       "       17.927671  , 19.85825   , 23.302856  ,  9.065862  , 15.903967  ,\n",
       "       39.097054  ,  1.0429688 ,  5.972855  , 22.769428  ,  3.631766  ,\n",
       "       27.198994  ,  6.765236  ,  0.29246822,  7.107895  , 12.230511  ,\n",
       "       17.532463  , 95.52748   ,  5.974314  , 16.589005  , 17.194849  ,\n",
       "       16.94459   , 64.80321   , 25.55472   ,  0.        ,  5.77121   ,\n",
       "        4.283899  , 19.353264  ,  3.1142168 , 17.487707  ,  7.445574  ,\n",
       "       10.285412  ,  5.131851  , 28.163557  ,  0.        , 17.232237  ,\n",
       "        0.        ,  5.1939583 , 16.76217   , 14.181374  ,  6.843934  ,\n",
       "        0.        ,  0.        , 15.084324  , 11.948604  , 30.011662  ,\n",
       "       33.492516  , 49.878708  ,  2.5962832 , 17.040329  , 18.788898  ,\n",
       "       35.07208   , 12.578891  ,  0.3588562 ,  3.7577844 ,  8.174013  ,\n",
       "       12.6843195 , 12.539602  , 16.441391  ,  0.        , 12.566673  ,\n",
       "       25.132092  ,  3.7659721 , 23.465189  , 18.830997  ,  1.3828099 ,\n",
       "       10.068995  ,  5.834901  ,  0.8979473 ,  1.5697422 ,  1.2954366 ,\n",
       "       10.079176  ,  0.8209853 ,  0.        , 18.10496   , 13.373021  ,\n",
       "       51.84065   , 19.006226  , 33.513947  ,  0.        , 10.149936  ,\n",
       "        7.619766  , 22.441988  , 18.945219  ,  6.6929665 ,  0.        ,\n",
       "        2.2428312 , 17.587406  ,  6.5397882 ,  0.5369301 ,  5.3317327 ,\n",
       "        2.347969  ,  0.        ,  2.285119  ,  9.035138  , 25.103804  ,\n",
       "       25.95662   , 10.713332  ,  5.0295296 , 16.499905  , 16.087229  ,\n",
       "        8.39431   , 19.269522  ,  6.934101  ,  1.3765371 , 21.421257  ,\n",
       "        2.5931075 , 21.784058  , 10.771777  ,  3.448063  , 38.8098    ,\n",
       "        4.8423643 , 19.991962  , 22.030941  ,  8.930917  ,  5.859139  ,\n",
       "       11.149387  ,  0.8129959 ,  0.        ,  0.        , 29.36559   ,\n",
       "        0.        ,  2.5914712 ,  0.        , 16.102043  ,  9.058838  ,\n",
       "       15.993111  ,  1.5180264 ,  6.3524046 ,  0.        ,  4.5758934 ,\n",
       "        4.9401975 ,  6.1751585 ,  0.        ,  7.1300306 ,  5.388287  ,\n",
       "        6.223568  ,  4.751236  ,  0.79574585,  0.        ,  0.8000183 ,\n",
       "       23.305782  , 30.293331  , 10.751662  ,  4.603733  , 19.80287   ,\n",
       "       10.2086735 , 17.962746  , 29.73349   ,  6.107295  ,  8.904945  ,\n",
       "        0.        , 12.416899  , 23.678875  , 49.02646   , 14.40205   ,\n",
       "        3.8598862 ,  0.        ,  0.54275227,  0.6714012 , 24.344742  ,\n",
       "        0.        ,  7.8763614 ,  8.423868  , 41.56363   , 17.277456  ,\n",
       "        7.167787  ,  8.427368  ,  6.706919  ,  7.044943  ,  0.8410754 ,\n",
       "        0.7958565 , 31.452919  ,  0.        ,  9.634126  , 16.201202  ,\n",
       "        4.241269  ,  3.5452423 ,  2.4684029 , 22.470282  ,  4.638485  ,\n",
       "        9.768917  , 12.155861  , 16.892189  ,  1.3367672 ,  5.106424  ,\n",
       "       15.154791  ,  1.3660936 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
