{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 17:56:42.973835: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-31 17:56:43.335441: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-31 17:56:43.343399: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-31 17:56:43.343449: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-31 17:56:44.989834: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-31 17:56:44.989948: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-31 17:56:44.989959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32, 64, 128, 256]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.0, 0.15, 0.3, 0.45]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'tanh']))\n",
    "HP_LR = hp.HParam('learning_rate', hp.Discrete([0.0001, 0.001, 0.01, 0.1]))\n",
    "HP_NUM_LAYERS = hp.HParam('num_layers', hp.Discrete([0, 1, 2, 3, 4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(config):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(config[HP_NUM_UNITS], activation=config[HP_ACTIVATION], input_shape=(512,)))\n",
    "\n",
    "  for i in range(config[HP_NUM_LAYERS]):\n",
    "    model.add(Dense(config[HP_NUM_UNITS], activation=config[HP_ACTIVATION]))\n",
    "\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  if config[HP_OPTIMIZER] == 'adam':\n",
    "    optimizer = Adam(learning_rate=config[HP_LR])\n",
    "  elif config[HP_OPTIMIZER] == 'sgd':\n",
    "    optimizer = SGD(learning_rate=config[HP_LR])\n",
    "  else:\n",
    "    optimizer = RMSprop(learning_rate=config[HP_LR])\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance \n",
    "def linear_diff(np1, np2):\n",
    "  np1 = np1.flatten()\n",
    "  np2 = np2.flatten()\n",
    "\n",
    "  return np1 - np2\n",
    "\n",
    "# TODO - also try absolute difference, cosine similarity, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7308, 2, 512)\n",
      "(7308, 512)\n",
      "(7308,)\n"
     ]
    }
   ],
   "source": [
    "features = np.load(\"./train_X.npy\")\n",
    "trainY = np.load(\"./train_y.npy\")\n",
    "\n",
    "print(features.shape)\n",
    "trainX = []\n",
    "for i in range(0, len(features)):\n",
    "  trainX.append(linear_diff(features[i][0], features[i][1]))\n",
    "\n",
    "trainX = np.array(trainX)\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "indices = np.arange(trainX.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trainX = trainX[indices]  \n",
    "trainY = trainY[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the eud version\n",
    "\n",
    "np.save(\"./train_X_diff.npy\", trainX)\n",
    "np.save(\"./train_y_diff.npy\", trainY)\n",
    "\n",
    "# Load the diff version\n",
    "# trainX = np.load(\"./train_X_diff.npy\")\n",
    "# trainY = np.load(\"./train_y_diff.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringifyConfig(config):\n",
    "  return f\"numLayers-{config[HP_NUM_LAYERS]}_numUnits-{config[HP_NUM_UNITS]}_dropout-{config[HP_DROPOUT]}_optimizer-{config[HP_OPTIMIZER]}_activation-{config[HP_ACTIVATION]}_lr-{config[HP_LR]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "modelID:  1704067059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 17:57:39.783405: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-12-31 17:57:39.783442: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-12-31 17:57:39.783478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (blankk): /proc/driver/nvidia/version does not exist\n",
      "2023-12-31 17:57:39.784063: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  18561\n",
      "HP_NUM_UNITS:  16\n",
      "HP_NUM_LAYERS:  0\n",
      "Epoch 1/100\n",
      "179/183 [============================>.] - ETA: 0s - loss: 0.7968 - accuracy: 0.6360\n",
      "Epoch 1: val_loss improved from inf to 0.55198, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-32_dropout-0.2_optimizer-adam_activation-relu_lr-0.0011704067059.h5\n",
      "183/183 [==============================] - 3s 12ms/step - loss: 0.7919 - accuracy: 0.6386 - val_loss: 0.5520 - val_accuracy: 0.7360\n",
      "Epoch 2/100\n",
      "175/183 [===========================>..] - ETA: 0s - loss: 0.3191 - accuracy: 0.8680\n",
      "Epoch 2: val_loss improved from 0.55198 to 0.38887, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-32_dropout-0.2_optimizer-adam_activation-relu_lr-0.0011704067059.h5\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.3183 - accuracy: 0.8678 - val_loss: 0.3889 - val_accuracy: 0.8393\n",
      "Epoch 3/100\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1601 - accuracy: 0.9502\n",
      "Epoch 3: val_loss improved from 0.38887 to 0.33869, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-32_dropout-0.2_optimizer-adam_activation-relu_lr-0.0011704067059.h5\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.1602 - accuracy: 0.9501 - val_loss: 0.3387 - val_accuracy: 0.8680\n",
      "Epoch 4/100\n",
      "174/183 [===========================>..] - ETA: 0s - loss: 0.0821 - accuracy: 0.9788\n",
      "Epoch 4: val_loss improved from 0.33869 to 0.31116, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-32_dropout-0.2_optimizer-adam_activation-relu_lr-0.0011704067059.h5\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0830 - accuracy: 0.9781 - val_loss: 0.3112 - val_accuracy: 0.8878\n",
      "Epoch 5/100\n",
      "177/183 [============================>.] - ETA: 0s - loss: 0.0437 - accuracy: 0.9921\n",
      "Epoch 5: val_loss improved from 0.31116 to 0.26911, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-32_dropout-0.2_optimizer-adam_activation-relu_lr-0.0011704067059.h5\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0440 - accuracy: 0.9916 - val_loss: 0.2691 - val_accuracy: 0.9083\n",
      "Epoch 6/100\n",
      "179/183 [============================>.] - ETA: 0s - loss: 0.0206 - accuracy: 0.9974\n",
      "Epoch 6: val_loss did not improve from 0.26911\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0205 - accuracy: 0.9974 - val_loss: 0.2931 - val_accuracy: 0.9131\n",
      "Epoch 7/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9991\n",
      "Epoch 7: val_loss did not improve from 0.26911\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0131 - accuracy: 0.9991 - val_loss: 0.2958 - val_accuracy: 0.9152\n",
      "Epoch 8/100\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9997\n",
      "Epoch 8: val_loss did not improve from 0.26911\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0066 - accuracy: 0.9997 - val_loss: 0.2934 - val_accuracy: 0.9193\n",
      "Epoch 9/100\n",
      "178/183 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9960\n",
      "Epoch 9: val_loss did not improve from 0.26911\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0161 - accuracy: 0.9959 - val_loss: 0.3582 - val_accuracy: 0.8981\n",
      "Epoch 10/100\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1049 - accuracy: 0.9665\n",
      "Epoch 10: val_loss did not improve from 0.26911\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.1041 - accuracy: 0.9668 - val_loss: 0.4335 - val_accuracy: 0.8762\n",
      "Epoch 11/100\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9705\n",
      "Epoch 11: val_loss did not improve from 0.26911\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0897 - accuracy: 0.9704 - val_loss: 0.4136 - val_accuracy: 0.9029\n",
      "Epoch 12/100\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9890\n",
      "Epoch 12: val_loss improved from 0.26911 to 0.22790, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-32_dropout-0.2_optimizer-adam_activation-relu_lr-0.0011704067059.h5\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0364 - accuracy: 0.9891 - val_loss: 0.2279 - val_accuracy: 0.9371\n",
      "Epoch 13/100\n",
      "177/183 [============================>.] - ETA: 0s - loss: 0.0140 - accuracy: 0.9961\n",
      "Epoch 13: val_loss did not improve from 0.22790\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0137 - accuracy: 0.9962 - val_loss: 0.2388 - val_accuracy: 0.9357\n",
      "Epoch 14/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 0.9993\n",
      "Epoch 14: val_loss improved from 0.22790 to 0.22389, saving model to ./models/flatten&eudlidian/numLayers-2_numUnits-32_dropout-0.2_optimizer-adam_activation-relu_lr-0.0011704067059.h5\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 0.2239 - val_accuracy: 0.9371\n",
      "Epoch 15/100\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 15: val_loss did not improve from 0.22389\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2529 - val_accuracy: 0.9425\n",
      "Epoch 16/100\n",
      "181/183 [============================>.] - ETA: 0s - loss: 9.4282e-04 - accuracy: 1.0000\n",
      "Epoch 16: val_loss did not improve from 0.22389\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 9.3877e-04 - accuracy: 1.0000 - val_loss: 0.2577 - val_accuracy: 0.9398\n",
      "Epoch 17/100\n",
      "174/183 [===========================>..] - ETA: 0s - loss: 6.5112e-04 - accuracy: 1.0000\n",
      "Epoch 17: val_loss did not improve from 0.22389\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 6.4955e-04 - accuracy: 1.0000 - val_loss: 0.2700 - val_accuracy: 0.9391\n",
      "Epoch 18/100\n",
      "176/183 [===========================>..] - ETA: 0s - loss: 5.1391e-04 - accuracy: 1.0000\n",
      "Epoch 18: val_loss did not improve from 0.22389\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 5.1165e-04 - accuracy: 1.0000 - val_loss: 0.2788 - val_accuracy: 0.9425\n",
      "Epoch 19/100\n",
      "178/183 [============================>.] - ETA: 0s - loss: 4.2519e-04 - accuracy: 1.0000\n",
      "Epoch 19: val_loss did not improve from 0.22389\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 4.1949e-04 - accuracy: 1.0000 - val_loss: 0.2797 - val_accuracy: 0.9432\n",
      "Epoch 20/100\n",
      "177/183 [============================>.] - ETA: 0s - loss: 3.4773e-04 - accuracy: 1.0000\n",
      "Epoch 20: val_loss did not improve from 0.22389\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 3.5048e-04 - accuracy: 1.0000 - val_loss: 0.2870 - val_accuracy: 0.9425\n",
      "Epoch 21/100\n",
      "175/183 [===========================>..] - ETA: 0s - loss: 3.0080e-04 - accuracy: 1.0000\n",
      "Epoch 21: val_loss did not improve from 0.22389\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 2.9686e-04 - accuracy: 1.0000 - val_loss: 0.2888 - val_accuracy: 0.9439\n",
      "Epoch 22/100\n",
      "179/183 [============================>.] - ETA: 0s - loss: 2.5385e-04 - accuracy: 1.0000\n",
      "Epoch 22: val_loss did not improve from 0.22389\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 2.5392e-04 - accuracy: 1.0000 - val_loss: 0.2920 - val_accuracy: 0.9432\n",
      "Epoch 23/100\n",
      "177/183 [============================>.] - ETA: 0s - loss: 2.1889e-04 - accuracy: 1.0000\n",
      "Epoch 23: val_loss did not improve from 0.22389\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 2.1927e-04 - accuracy: 1.0000 - val_loss: 0.2948 - val_accuracy: 0.9432\n",
      "Epoch 24/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.9045e-04 - accuracy: 1.0000\n",
      "Epoch 24: val_loss did not improve from 0.22389\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 1.9045e-04 - accuracy: 1.0000 - val_loss: 0.3004 - val_accuracy: 0.9432\n"
     ]
    }
   ],
   "source": [
    "method = \"flatten&eudlidian\"\n",
    "\n",
    "for NUM_UNIT in HP_NUM_UNITS.domain.values:\n",
    "  for NUM_LAYER in HP_NUM_LAYERS.domain.values:\n",
    "\n",
    "    # config = {\n",
    "    #   HP_NUM_UNITS: NUM_UNIT,\n",
    "    #   HP_DROPOUT: 0,\n",
    "    #   HP_OPTIMIZER: 'adam',\n",
    "    #   HP_ACTIVATION: 'relu',\n",
    "    #   HP_LR: 0.001,\n",
    "    #   HP_NUM_LAYERS: NUM_LAYER\n",
    "    # }\n",
    "\n",
    "    config = {\n",
    "      HP_NUM_UNITS: 32,\n",
    "      HP_DROPOUT: 0.2,\n",
    "      HP_OPTIMIZER: 'adam',\n",
    "      HP_ACTIVATION: 'relu',\n",
    "      HP_LR: 0.001,\n",
    "      HP_NUM_LAYERS: 2\n",
    "    }\n",
    "\n",
    "    # Callbacks\n",
    "    modelID = int(time.time())\n",
    "    modelName = stringifyConfig(config) + str(modelID)\n",
    "\n",
    "    print(\"\\nmodelID: \", modelID)\n",
    "\n",
    "    # Callbacks\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=f\"./logs/{method}/{modelName}\", histogram_freq=1)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\"./models/\" + method + \"/\" + modelName + \".h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "\n",
    "    hparms = hp.KerasCallback(\"./logs/\" + method + \"/\" + modelName, config)\n",
    "\n",
    "    callbacks_list = [checkpoint, early, tensorboard_callback, hparms]\n",
    "\n",
    "    model = createModel(config)\n",
    "\n",
    "    print(\"Parameters: \", model.count_params())\n",
    "    print(\"HP_NUM_UNITS: \", NUM_UNIT)\n",
    "    print(\"HP_NUM_LAYERS: \", NUM_LAYER)\n",
    "\n",
    "    model.fit(trainX, trainY, batch_size=32, epochs=100, validation_split=0.2, callbacks=callbacks_list, shuffle=True, verbose=1)\n",
    "\n",
    "    break\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = keras.models.load_model(\"./models/\" + method + \"/\" + modelName + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1848, 512)\n",
      "(1848,)\n",
      "Test loss: 0.998153805732727\n",
      "Test accuracy: 0.7819263935089111\n"
     ]
    }
   ],
   "source": [
    "textY = np.load(\"./test_y.npy\") \n",
    "textX = []\n",
    "features = np.load(\"./test_X.npy\")\n",
    "for i in range(0, len(features)):\n",
    "  textX.append(linear_diff(features[i][0], features[i][1]))\n",
    "\n",
    "textX = np.array(textX)\n",
    "\n",
    "print(textX.shape)\n",
    "print(textY.shape)\n",
    "\n",
    "score = model.evaluate(textX, textY, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
